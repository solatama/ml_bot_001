import os
import time
import logging
import numpy as np
import pandas as pd
from math import factorial
import matplotlib.pyplot as plt
import talib
import yfinance as yf
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import log_loss
from sklearn.ensemble import RandomForestClassifier, StackingClassifier, VotingClassifier
from sklearn.neural_network import MLPClassifier
from scipy.stats import ttest_1samp
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostClassifier
import optuna
from optuna.pruners import MedianPruner
from tqdm import tqdm

# === å®šæ•°å®šç¾© ===
TICKER = '9684.T'
START_DATE = '2024-04-01'
END_DATE = '2025-04-10'
INTERVAL = '1d'
FEATURES = ['close_scaled']
STOP_LOSS = 0.02
TAKE_PROFIT = 0.05
COMMISSION = 0.001
SLIPPAGE = 0.001
SELECTED_MODELS = ['xgboost', 'randomforest', 'catboost', 'lightgbm'] #'lightgbm',
ENSEMBLE_TYPE = 'stacking'  # 'blending', 'stacking', 'voting_hard', 'voting_soft'

# === 1. ãƒ‡ãƒ¼ã‚¿å–å¾—é–¢æ•° ===
def get_data(ticker, start_date, end_date, interval='1d'):
    try:
        df = yf.download(ticker, start=start_date, end=end_date, interval=interval)
        df = df[['Open', 'High', 'Low', 'Close', 'Volume']]
        if isinstance(df.columns, pd.MultiIndex):
            df.columns = df.columns.get_level_values(0)
        df.columns = [col.lower() for col in df.columns]
        return df
    except Exception as e:
        print(f"[ã‚¨ãƒ©ãƒ¼] ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        raise  # ã‚¨ãƒ©ãƒ¼ã‚’ãã®ã¾ã¾æŠ•ã’ã¦å‡¦ç†ã‚’ä¸­æ–­

# === 2. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†é–¢æ•° ===
def add_cumret(df):
    df['cum_ret'] = df['close'].pct_change().cumsum()
    return df

#=== 3. ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°é–¢æ•° ===
def scale_features(df, features):
    scaler = MinMaxScaler()
    df_scaled = df.copy()
    df_scaled[features] = scaler.fit_transform(df[features])
    return df_scaled

def basic_preprocessing(df):
    from sklearn.preprocessing import MinMaxScaler

    scaler = MinMaxScaler()
    df['close_scaled'] = scaler.fit_transform(df[['close']])
    return df

# === 4. ç‰¹å¾´é‡ç”Ÿæˆ ===
def calc_features(df):
    required_columns = ['open', 'high', 'low', 'close', 'volume']
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        raise ValueError(f"å¿…è¦ãªåˆ—ãŒãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å­˜åœ¨ã—ã¾ã›ã‚“: {missing_columns}")

    # å¿…è¦ãªã‚«ãƒ©ãƒ ã‚’å–å¾—ã—ã€numpy.ndarray å‹ã«å¤‰æ›
    open_col = df['open'].values.astype(float)
    high_col = df['high'].values.astype(float)
    low_col = df['low'].values.astype(float)
    close_col = df['close'].values.astype(float)
    volume = df['volume'].values.astype(float)

    orig_columns = df.columns

    hilo = (df['high'] + df['low']) / 2
    # ä¾¡æ ¼(hilo ã¾ãŸã¯ close)ã‚’å¼•ã„ãŸå¾Œã€ä¾¡æ ¼(close)ã§å‰²ã‚‹ã“ã¨ã§æ¨™æº–åŒ–ã—ã¦ã‚‹ã‚‚ã®ã‚ã‚Š

    # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰
    df['BBANDS_upperband'], df['BBANDS_middleband'], df['BBANDS_lowerband'] = talib.BBANDS(df['close'], timeperiod=5, nbdevup=2, nbdevdn=2, matype=0)
    df['BBANDS_upperband'] = (df['BBANDS_upperband'] - hilo) / df['close']
    df['BBANDS_middleband'] = (df['BBANDS_middleband'] - hilo) / df['close']
    df['BBANDS_lowerband'] = (df['BBANDS_lowerband'] - hilo) / df['close']

    # ç§»å‹•å¹³å‡
    df['DEMA'] = (talib.DEMA(close_col, timeperiod=30) - hilo) / close_col
    df['EMA'] = (talib.EMA(close_col, timeperiod=30) - hilo) / close_col
    df['EMA_short'] = (talib.EMA(close_col, timeperiod=5) - hilo) / close_col
    df['EMA_middle'] = (talib.EMA(close_col, timeperiod=20) - hilo) / close_col
    df['EMA_long'] = (talib.EMA(close_col, timeperiod=40) - hilo) / close_col
    df['HT_TRENDLINE'] = (talib.HT_TRENDLINE(close_col) - hilo) / close_col
    df['KAMA'] = (talib.KAMA(close_col, timeperiod=30) - hilo) / close_col
    df['MA'] = (talib.MA(close_col, timeperiod=30, matype=0) - hilo) / close_col
    df['MIDPOINT'] = (talib.MIDPOINT(close_col, timeperiod=14) - hilo) / close_col
    df['SMA'] = (talib.SMA(close_col, timeperiod=30) - hilo) / close_col
    df['T3'] = (talib.T3(close_col, timeperiod=5, vfactor=0) - hilo) / close_col
    df['HMA'] = talib.WMA(close_col, timeperiod=30)
    df['TEMA'] = (talib.TEMA(close_col, timeperiod=30) - hilo) / close_col
    df['TRIMA'] = (talib.TRIMA(close_col, timeperiod=30) - hilo) / close_col
    df['WMA'] = (talib.WMA(close_col, timeperiod=30) - hilo) / close_col

    # MACD
    df['MACD_macd'], df['MACD_macdsignal'], df['MACD_macdhist'] = talib.MACD(close_col, fastperiod=12, slowperiod=26, signalperiod=9) # Use close_col instead of close
    df['MACD_macd'] /= close_col # Use close_col instead of close
    df['MACD_macdsignal'] /= close_col # Use close_col instead of close
    df['MACD_macdhist'] /= close_col # Use close_col instead of close
    df['MACD_EXT'], df['MACD_SIGNAL_EXT'], df['MACD_HIST_EXT'] = talib.MACDEXT(close_col, fastperiod=12, slowperiod=26, signalperiod=9, fastmatype=0, slowmatype=0, signalmatype=0) # Use close_col instead of close

    # ç·šå½¢å›å¸°ç³»
    df['LINEARREG'] = (talib.LINEARREG(close_col, timeperiod=14) - close_col) / close_col # Use close_col instead of close
    df['LINEARREG_SLOPE'] = talib.LINEARREG_SLOPE(close_col, timeperiod=14) / close_col # Use close_col instead of close
    df['LINEARREG_ANGLE'] = talib.LINEARREG_ANGLE(close_col, timeperiod=14) # Use close_col instead of close
    df['LINEARREG_INTERCEPT'] = (talib.LINEARREG_INTERCEPT(close_col, timeperiod=14) - close_col) / close_col # Use close_col instead of close

    # ADç³»
    df['AD'] = talib.AD(high_col, low_col, close_col, volume) / close_col # Use high_col, low_col, close_col instead of high, low, close
    df['ADX'] = talib.ADX(high_col, low_col, close_col, timeperiod=14) # Use high_col, low_col, close_col instead of high, low, close
    df['ADXR'] = talib.ADXR(high_col, low_col, close_col, timeperiod=14) # Use high_col, low_col, close_col instead of high, low, close
    df['ADOSC'] = talib.ADOSC(high_col, low_col, close_col, volume, fastperiod=3, slowperiod=10) / close_col # Use high_col, low_col, close_col instead of high, low, close
    df['OBV'] = talib.OBV(close_col, volume) / close_col # Use close_col instead of close

    # ã‚ªã‚·ãƒ¬ãƒ¼ã‚¿ãƒ¼ç³»
    df['APO'] = talib.APO(close_col, fastperiod=12, slowperiod=26, matype=0) / close_col  # Changed close to close_col
    df['BOP'] = talib.BOP(open_col, high_col, low_col, close_col)  # Changed open, high, low, close to their respective _col variables
    df['CCI'] = talib.CCI(high_col, low_col, close_col, timeperiod=14)  # Changed high, low, close to their respective _col variables
    df['DX'] = talib.DX(high_col, low_col, close_col, timeperiod=14)  # Changed high, low, close to their respective _col variables
    df['MFI'] = talib.MFI(high_col, low_col, close_col, volume, timeperiod=14)  # Changed high, low, close to their respective _col variables
    df['MINUS_DI'] = talib.MINUS_DI(high_col, low_col, close_col, timeperiod=14)  # Changed high, low, close to their respective _col variables
    df['PLUS_DI'] = talib.PLUS_DI(high_col, low_col, close_col, timeperiod=14)  # Changed high, low, close to their respective _col variables
    df['MOM'] = talib.MOM(close_col, timeperiod=10) / close_col  # Changed close to close_col
    df['RSI'] = talib.RSI(close_col, timeperiod=14)  # Changed close to close_col
    df['TRIX'] = talib.TRIX(close_col, timeperiod=30)  # Changed close to close_col
    df['ULTOSC'] = talib.ULTOSC(high_col, low_col, close_col, timeperiod1=7, timeperiod2=14, timeperiod3=28)  # Changed high, low, close to their respective _col variables
    df['WILLR'] = talib.WILLR(high_col, low_col, close_col, timeperiod=14)  # Changed high, low, close to their respective _col variables
    df['SAR'] = talib.SAR(high_col, low_col, acceleration=0.02, maximum=0.2)  # Changed high, low to their respective _col variables

    # ã‚¹ãƒˆã‚­ãƒ£ã‚¹ãƒ†ã‚£ã‚¯ã‚¹
    df['STOCH_slowk'], df['STOCH_slowd'] = talib.STOCH(high_col, low_col, close_col, fastk_period=5, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0) # Changed high, low, close to high_col, low_col, close_col
    df['STOCHF_fastk'], df['STOCHF_fastd'] = talib.STOCHF(high_col, low_col, close_col, fastk_period=5, fastd_period=3, fastd_matype=0) # Changed high, low, close to high_col, low_col, close_col
    df['STOCHRSI_fastk'], df['STOCHRSI_fastd'] = talib.STOCHRSI(close_col, timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0) # Changed close to close_col

    # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç³»
    df['MINUS_DM'] = talib.MINUS_DM(high_col, low_col, timeperiod=14) / close_col # Changed high, low, close to high_col, low_col, close_col
    df['PLUS_DM'] = talib.PLUS_DM(high_col, low_col, timeperiod=14) / close_col # Changed high, low, close to high_col, low_col, close_col
    df['STDDEV'] = talib.STDDEV(close_col, timeperiod=5, nbdev=1) # Changed close to close_col
    df['TRANGE'] = talib.TRANGE(high_col, low_col, close_col) # Changed high, low, close to high_col, low_col, close_col
    df['VAR'] = talib.VAR(close_col, timeperiod=5, nbdev=1) # Changed close to close_col
    df['ATR'] = talib.ATR(high_col, low_col, close_col, timeperiod=14) # Changed high, low, close to high_col, low_col, close_col
    df['NATR'] = talib.NATR(high_col, low_col, close_col, timeperiod=14) # Changed high, low, close to high_col, low_col, close_col
    df['VOLATILITY_index'] = df['ATR'] / df['STDDEV']

    # ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›
    df['HT_DCPERIOD'] = talib.HT_DCPERIOD(close_col) # Changed close to close_col
    df['HT_DCPHASE'] = talib.HT_DCPHASE(close_col) # Changed close to close_col
    df['HT_PHASOR_inphase'], df['HT_PHASOR_quadrature'] = talib.HT_PHASOR(close_col) # Changed close to close_col
    df['HT_PHASOR_inphase'] /= close_col # Changed close to close_col
    df['HT_PHASOR_quadrature'] /= close_col # Changed close to close_col
    df['HT_SINE_sine'], df['HT_SINE_leadsine'] = talib.HT_SINE(close_col) # Changed close to close_col
    df['HT_SINE_sine'] /= close_col # Changed close to close_col
    df['HT_SINE_leadsine'] /= close_col # Changed close to close_col
    df['HT_TRENDMODE'] = talib.HT_TRENDMODE(close_col) # Changed close to close_col

    # ãã®ä»–
    df['ROC'] = talib.ROC(close_col, timeperiod=10) / close_col # Changed close to close_col
    df['STDDEV'] = talib.STDDEV(close_col, timeperiod=5, nbdev=1) / close_col # Changed close to close_col
    df['TRANGE'] = talib.TRANGE(high_col, low_col, close_col) / close_col # Changed high, low, close to high_col, low_col, close_col
    df['AROON_aroondown'], df['AROON_aroonup'] = talib.AROON(high_col, low_col, timeperiod=14) # Changed high, low to high_col, low_col
    df['AROONOSC'] = talib.AROONOSC(high_col, low_col, timeperiod=14) # Changed high, low to high_col, low_col
    df['BETA'] = talib.BETA(high_col, low_col, timeperiod=5) # Changed high, low to high_col, low_col
    df['CORREL'] = talib.CORREL(high_col, low_col, timeperiod=30) # Changed high, low to high_col, low_col
    df['Price_ratio'] = df['close'] / df['close'].shift(1)  # Changed df[close_col] to df['close']
    df['HIGH_ratio'] = df['high'] / df['high'].shift(1)  # Changed df[high_col] to df['high']
    df['LOW_ratio'] = df['low'] / df['low'].shift(1)  # Changed df[low_col] to df['low']

    # Lagç‰¹å¾´é‡
    lags = [1, 3, 5, 10, 20]
    for lag in lags:
        df[f'close_lag_{lag}'] = df['close'].shift(lag)
        df[f'return_lag_{lag}'] = df['close'].pct_change(lag)
        df[f'log_return_lag_{lag}'] = np.log(df['close'] / df['close'].shift(lag))
    # å‘¨æœŸæ€§ã®ç‰¹å¾´é‡
    df['DAY_of_week'] = df.index.dayofweek  # æ›œæ—¥ï¼ˆ0=æœˆæ›œæ—¥, 6=æ—¥æ›œæ—¥ï¼‰
    df['IS_weekend'] = (df['DAY_of_week'] >= 5).astype(int)  # é€±æœ«ã‹ã©ã†ã‹
    df['MONTH'] = df.index.month  # æœˆï¼ˆ1ã€œ12ï¼‰
    df['SIN_day'] = np.sin(2 * np.pi * df['DAY_of_week'] / 7)  # æ—¥å‘¨æœŸ
    df['COS_day'] = np.cos(2 * np.pi * df['DAY_of_week'] / 7)  # æ—¥å‘¨æœŸ
    df['SIN_month'] = np.sin(2 * np.pi * df['MONTH'] / 12)  # æœˆå‘¨æœŸ
    df['COS_month'] = np.cos(2 * np.pi * df['MONTH'] / 12)  # æœˆå‘¨æœŸ

    # ãƒªã‚¿ãƒ¼ãƒ³ç³»
    df['log_return'] = np.log(df['close'] / df['close'].shift(1))
    df['return_1d'] = df['close'].pct_change(1)
    df['return_5d'] = df['close'].pct_change(5)
    df['return_10d'] = df['close'].pct_change(10)
    df['return_20d'] = df['close'].pct_change(20)

    df['return_ma_5'] = df['return_1d'].rolling(window=5).mean()
    df['return_ma_10'] = df['return_1d'].rolling(window=10).mean()

    df['volatility_5'] = df['close'].rolling(window=5).std()
    df['volatility_10'] = df['close'].rolling(window=10).std()

    df['range'] = df['high'] - df['low']

    df['volume_change'] = df['volume'].pct_change(1)

    # ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ç³»
    df['ma_5'] = df['close'].rolling(window=5).mean()
    df['ma_25'] = df['close'].rolling(window=25).mean()
    df['ma_75'] = df['close'].rolling(window=75).mean()

    df['ma_deviation_5'] = df['close'] / df['ma_5'] - 1
    df['ma_deviation_25'] = df['close'] / df['ma_25'] - 1
    df['ma_deviation_75'] = df['close'] / df['ma_75'] - 1

    # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ï¼ˆnæ—¥å‰æ¯”ï¼‰
    df['momentum_5'] = df['close'] - df['close'].shift(5)
    df['momentum_10'] = df['close'] - df['close'].shift(10)
    df['momentum_20'] = df['close'] - df['close'].shift(20)

    # --- ãƒ­ãƒ¼ã‚½ã‚¯è¶³ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ50ç¨®é¡ï¼‰---
    # å€¤ã¯ +100: å¼·æ°—ã‚·ã‚°ãƒŠãƒ«, -100: å¼±æ°—ã‚·ã‚°ãƒŠãƒ«, 0: ã‚·ã‚°ãƒŠãƒ«ãªã—
    candlestick_patterns = {
        'CDL2CROWS': talib.CDL2CROWS,
        'CDL3BLACKCROWS': talib.CDL3BLACKCROWS,
        'CDL3INSIDE': talib.CDL3INSIDE,
        'CDL3LINESTRIKE': talib.CDL3LINESTRIKE,
        'CDL3OUTSIDE': talib.CDL3OUTSIDE,
        'CDL3STARSINSOUTH': talib.CDL3STARSINSOUTH,
        'CDL3WHITESOLDIERS': talib.CDL3WHITESOLDIERS,
        'CDLABANDONEDBABY': talib.CDLABANDONEDBABY,
        'CDLADVANCEBLOCK': talib.CDLADVANCEBLOCK,
        'CDLBELTHOLD': talib.CDLBELTHOLD,
        'CDLBREAKAWAY': talib.CDLBREAKAWAY,
        'CDLCLOSINGMARUBOZU': talib.CDLCLOSINGMARUBOZU,
        'CDLCONCEALBABYSWALL': talib.CDLCONCEALBABYSWALL,
        'CDLCOUNTERATTACK': talib.CDLCOUNTERATTACK,
        'CDLDARKCLOUDCOVER': talib.CDLDARKCLOUDCOVER,
        'CDLDOJI': talib.CDLDOJI,
        'CDLDOJISTAR': talib.CDLDOJISTAR,
        'CDLDRAGONFLYDOJI': talib.CDLDRAGONFLYDOJI,
        'CDLENGULFING': talib.CDLENGULFING,
        'CDLEVENINGDOJISTAR': talib.CDLEVENINGDOJISTAR,
        'CDLEVENINGSTAR': talib.CDLEVENINGSTAR,
        'CDLGAPSIDESIDEWHITE': talib.CDLGAPSIDESIDEWHITE,
        'CDLGRAVESTONEDOJI': talib.CDLGRAVESTONEDOJI,
        'CDLHAMMER': talib.CDLHAMMER,
        'CDLHANGINGMAN': talib.CDLHANGINGMAN,
        'CDLHARAMI': talib.CDLHARAMI,
        'CDLHARAMICROSS': talib.CDLHARAMICROSS,
        'CDLHIGHWAVE': talib.CDLHIGHWAVE,
        'CDLHIKKAKE': talib.CDLHIKKAKE,
        'CDLHIKKAKEMOD': talib.CDLHIKKAKEMOD,
        'CDLHOMINGPIGEON': talib.CDLHOMINGPIGEON,
        'CDLIDENTICAL3CROWS': talib.CDLIDENTICAL3CROWS,
        'CDLINNECK': talib.CDLINNECK,
        'CDLINVERTEDHAMMER': talib.CDLINVERTEDHAMMER,
        'CDLKICKING': talib.CDLKICKING,
        'CDLKICKINGBYLENGTH': talib.CDLKICKINGBYLENGTH,
        'CDLLADDERBOTTOM': talib.CDLLADDERBOTTOM,
        'CDLLONGLEGGEDDOJI': talib.CDLLONGLEGGEDDOJI,
        'CDLLONGLINE': talib.CDLLONGLINE,
        'CDLMARUBOZU': talib.CDLMARUBOZU,
        'CDLMATCHINGLOW': talib.CDLMATCHINGLOW,
        'CDLMATHOLD': talib.CDLMATHOLD,
        'CDLMORNINGDOJISTAR': talib.CDLMORNINGDOJISTAR,
        'CDLMORNINGSTAR': talib.CDLMORNINGSTAR,
        'CDLONNECK': talib.CDLONNECK,
        'CDLPIERCING': talib.CDLPIERCING,
        'CDLRICKSHAWMAN': talib.CDLRICKSHAWMAN,
        'CDLRISEFALL3METHODS': talib.CDLRISEFALL3METHODS,
        'CDLSEPARATINGLINES': talib.CDLSEPARATINGLINES,
        'CDLSHOOTINGSTAR': talib.CDLSHOOTINGSTAR,
    }

    # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä¸€æ‹¬ã§è¨ˆç®—ã—ã¦ DataFrame ã«è¿½åŠ 
    for name, func in candlestick_patterns.items():
        df[name] = func(open_col, high_col, low_col, close_col)

    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã•ã‚ŒãŸ 'close' åˆ—ã‚’è¿½åŠ 
    scaler = MinMaxScaler()
    df['close_scaled'] = scaler.fit_transform(df[['close']])

    # ä¾‹: ç¿Œæ—¥ã®çµ‚å€¤ãŒå½“æ—¥ã®çµ‚å€¤ã‚ˆã‚Šé«˜ã‘ã‚Œã°1ã€ãã†ã§ãªã‘ã‚Œã°0
    df['long_target'] = (df['close'].shift(-1) > df['close']).astype(int)

    df.dropna(inplace=True)
    return df

# === 5. ç›¸é–¢ä¿‚æ•°ã«ã‚ˆã‚‹ç‰¹å¾´é‡å‰Šé™¤é–¢æ•° ===
def remove_highly_correlated_features(df, threshold=0.9, exclude_columns=None):
    """
    é«˜ã„ç›¸é–¢ã‚’æŒã¤ç‰¹å¾´é‡ã‚’å‰Šé™¤ã™ã‚‹ã€‚
    :param df: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    :param threshold: ç›¸é–¢ä¿‚æ•°ã®é–¾å€¤
    :param exclude_columns: å‰Šé™¤å¯¾è±¡ã‹ã‚‰é™¤å¤–ã™ã‚‹åˆ—ã®ãƒªã‚¹ãƒˆ
    :return: ç›¸é–¢ãŒé«˜ããªã„ç‰¹å¾´é‡ã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    if exclude_columns is None:
        exclude_columns = []

    # ç›¸é–¢è¡Œåˆ—ã‚’è¨ˆç®—
    corr_matrix = df.corr().abs()

    # ä¸Šä¸‰è§’è¡Œåˆ—ã‚’å–å¾—
    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

    # é«˜ç›¸é–¢ã®åˆ—ã‚’ç‰¹å®š
    to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold) and column not in exclude_columns]

    print(f"ğŸ› ï¸ å‰Šé™¤ã•ã‚ŒãŸé«˜ç›¸é–¢ç‰¹å¾´é‡: {to_drop}")

    # é«˜ç›¸é–¢ã®åˆ—ã‚’å‰Šé™¤
    return df.drop(columns=to_drop)

# === 6. LightGBM Sklearn Wrapper ===
class LightGBMSklearnWrapper(BaseEstimator, ClassifierMixin):
    def __init__(self, **params):
        import lightgbm as lgb
        self.model = lgb.LGBMClassifier(**params)

    def fit(self, X, y):
        self.model.fit(X, y)
        return self

    def predict(self, X):
        return self.model.predict(X)

    def predict_proba(self, X):
        return self.model.predict_proba(X)

# === 7. LightGBMé‡è¦åº¦ã§ä¸Šä½ç‰¹å¾´é‡ã‚’é¸æŠ ===
def select_top_features_with_lightgbm(df, target, top_n=10):
    """
    LightGBMã®é‡è¦åº¦ã‚’ç”¨ã„ã¦ä¸Šä½ã®ç‰¹å¾´é‡ã‚’é¸æŠã™ã‚‹ã€‚
    :param df: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    :param target: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—å
    :param top_n: é¸æŠã™ã‚‹ç‰¹å¾´é‡ã®æ•°
    :return: ä¸Šä½ç‰¹å¾´é‡ã®ãƒªã‚¹ãƒˆ
    """
    X = df.drop(columns=[target])
    y = df[target]

    model = lgb.LGBMClassifier()
    model.fit(X, y)

    feature_importances = pd.DataFrame({
        'feature': X.columns,
        'importance': model.feature_importances_
    }).sort_values(by='importance', ascending=False)

    top_features = feature_importances.head(top_n)['feature'].tolist()
    print(f"ğŸŒŸ LightGBMã§é¸ã°ã‚ŒãŸä¸Šä½ {top_n} ç‰¹å¾´é‡: {top_features}")
    return top_features

# === 8. tæ¤œå®šé–¢æ•° ===
def perform_t_test(df):
    x = df['cum_ret'].diff(1).dropna()  # ç´¯ç©ãƒªã‚¿ãƒ¼ãƒ³ã®å·®åˆ†ã‚’è¨ˆç®—
    t, p = ttest_1samp(x, 0)  # tæ¤œå®šã‚’å®Ÿè¡Œ
    return t, p

# === 9. på¹³å‡æ³•è¨ˆç®—é–¢æ•° ===
def calc_p_mean(x, n):
    ps = []
    for i in range(n):
        x2 = x[i * x.size // n:(i + 1) * x.size // n]
        if np.std(x2) == 0:
            ps.append(1)
        else:
            t, p = ttest_1samp(x2, 0)
            if t > 0:
                ps.append(p)
            else:
                ps.append(1)
    return np.mean(ps)

def calc_p_mean_type1_error_rate(p_mean, n):
    return (p_mean * n) ** n / math.factorial(n)

# === 10. ãƒ¢ãƒ‡ãƒ«ä½œæˆ ===
def create_model(model_type, params=None):
    if model_type == 'lightgbm':
        return lgb.LGBMClassifier(verbose=-1, **params)
    elif model_type == 'xgboost':
        return xgb.XGBClassifier(**params)
    elif model_type == 'catboost':
        return CatBoostClassifier(verbose=0, **params)
    elif model_type == 'randomforest':
        return RandomForestClassifier(**params)
    elif model_type == 'mlp':
        return MLPClassifier(max_iter=500, **params)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")

# === 10. ãƒ¢ãƒ‡ãƒ«ä½œæˆ
def create_base_models(selected_models, best_params_dict):
    return [(name, create_model(name, best_params_dict[name])) for name in selected_models]

# === 11. Optunaã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ– ===
def optimize_hyperparameters(df, model_type):
    def objective(trial):
        # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«ç•°ãªã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®š
        if model_type == 'lightgbm':
            params = {
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),
                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
                'max_depth': trial.suggest_int('max_depth', 3, 10),
            }
        elif model_type == 'xgboost':
            params = {
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),
                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
                'max_depth': trial.suggest_int('max_depth', 3, 10),
                'gamma': trial.suggest_float('gamma', 0, 5),
            }
        elif model_type == 'catboost':
            params = {
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),
                'depth': trial.suggest_int('depth', 3, 10),
                'iterations': trial.suggest_int('iterations', 100, 1000),
            }
        elif model_type == 'randomforest':
            params = {
                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
                'max_depth': trial.suggest_int('max_depth', 3, 10),
                'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),
                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),
            }
        else:
            raise ValueError(f"Unsupported model type for optimization: {model_type}")

        # ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        model = create_model(model_type, params)
        scores = []
        tscv = TimeSeriesSplit(n_splits=3)
        for train_idx, test_idx in tscv.split(df):
            X_train, X_test = df.iloc[train_idx][FEATURES], df.iloc[test_idx][FEATURES]
            y_train, y_test = df.iloc[train_idx]['long_target'], df.iloc[test_idx]['long_target']
            model.fit(X_train, y_train)
            preds = model.predict_proba(X_test)[:, 1]
            scores.append(log_loss(y_test, preds))
        return np.mean(scores)

    # Optunaã§æœ€é©åŒ–
    study = optuna.create_study(direction='minimize', pruner=MedianPruner())
    study.optimize(objective, n_trials=30)
    return study.best_params

# === 12. på¹³å‡æ³•ã«ã‚ˆã‚‹ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ãƒ¼æœ‰æ„æ€§æ¤œå®š ===
def p_mean_test(returns, period=14, alpha=0.03):
    """
    på¹³å‡æ³•ã«ã‚ˆã‚‹ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ãƒ¼æœ‰æ„æ€§æ¤œå®š
    Parameters:
        returns: ãƒªã‚¿ãƒ¼ãƒ³ã®æ™‚ç³»åˆ— (1æ¬¡å…ƒ array-like)
        period: æ¤œå®šã«ä½¿ã†æœŸé–“ã®é•·ã•ï¼ˆæ—¥æ•°ï¼‰
        alpha: æœ‰æ„æ°´æº– (ä¾‹: 0.03)
    Returns:
        mean_p: på€¤ã®å¹³å‡
        significant: æœ‰æ„ã‹ã©ã†ã‹ï¼ˆmean_p < alphaï¼‰
        error_rate: ã‚¨ãƒ©ãƒ¼ç‡ã®è¿‘ä¼¼å€¤
    """
    returns = np.array(returns)
    n = len(returns) // period
    p_values = []

    for i in range(n):
        chunk = returns[i*period : (i+1)*period]
        if len(chunk) < 2:  # tæ¤œå®šã«ã¯2ç‚¹ä»¥ä¸Šå¿…è¦
            continue
        _, p = ttest_1samp(chunk, popmean=0, alternative='greater')
        p_values.append(p)

    if not p_values:
        raise ValueError("æœ‰åŠ¹ãªæœŸé–“ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã¾ã™ã€‚")

    mean_p = np.mean(p_values)
    significant = mean_p < alpha

    # ã‚¨ãƒ©ãƒ¼ç‡ã®è¿‘ä¼¼ (mean(p)*N)^N / N!
    N = len(p_values)
    error_rate = (mean_p * N)**N / factorial(N)

    return mean_p, significant, error_rate

# === 13. ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ ===
from scipy.special import softmax

# === 14. ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆé–¢æ•° ===
def run_backtest(df, model, features):
    predictions = model.predict(df[features])
    position = None
    entry_price = 0
    pnl = []
    equity_curve = [1.0]

    for i, pred in enumerate(predictions):
        close_price = df.iloc[i]['close']

        if position is None and pred > 0.5:
            position = 'long'
            entry_price = close_price

        if position == 'long':
            stop_loss = entry_price * (1 - STOP_LOSS)
            take_profit = entry_price * (1 + TAKE_PROFIT)
            if close_price <= stop_loss or close_price >= take_profit:
                profit = close_price - entry_price - (entry_price * COMMISSION + close_price * SLIPPAGE)
                pnl.append(profit)
                position = None
                equity_curve.append(equity_curve[-1] * (1 + profit / entry_price))

    daily_returns = np.diff(equity_curve) / equity_curve[:-1]
    sharpe_ratio = np.mean(daily_returns) / np.std(daily_returns) * np.sqrt(252) if len(daily_returns) > 1 and np.std(daily_returns) > 0 else 0
    equity_curve_array = np.array(equity_curve)
    drawdown = equity_curve_array / np.maximum.accumulate(equity_curve_array) - 1
    max_drawdown = drawdown.min() if len(drawdown) > 0 else 0
    total_pnl = np.sum(pnl)
    win_rate = np.mean(np.array(pnl) > 0) if pnl else 0

    return {
        "total_pnl": total_pnl,
        "win_rate": win_rate,
        "sharpe_ratio": sharpe_ratio,
        "max_drawdown": max_drawdown,
        "equity_curve": equity_curve,
    }

# å¯è¦–åŒ–é–¢æ•°
def plot_equity_curve(df):
    plt.figure(figsize=(10, 5))
    plt.plot(df['cum_ret'], label='Equity Curve')
    plt.title('Equity Curve')
    plt.xlabel('Date')
    plt.ylabel('Cumulative Return')
    plt.legend()
    plt.grid()
    plt.show()

def plot_feature_heatmap(df, features):
    plt.figure(figsize=(10, 8))
    sns.heatmap(df[features].corr(), annot=True, cmap='coolwarm')
    plt.title('Feature Correlation Heatmap')
    plt.show()

# === 15. ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨è©•ä¾¡ ===
def train_and_evaluate_model(df, features, target, model_class, test_size=0.2):
    X = df[features]
    y = df[target]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

    model = model_class()
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    return model, accuracy

# === 16. ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰åˆ†æ ===
from sklearn.base import clone

def run_walk_forward_backtest(df, model, features, n_splits=5):
    tscv = TimeSeriesSplit(n_splits=n_splits)
    fold_results = []
    train_returns = []
    test_returns = []

    for train_idx, test_idx in tqdm(tscv.split(df)):
        train_df, test_df = df.iloc[train_idx], df.iloc[test_idx]

        cloned_model = clone(model)
        cloned_model.fit(train_df[features], train_df['long_target'])

        train_bt = run_backtest(train_df, cloned_model, features)
        train_equity = np.array(train_bt['equity_curve'])
        train_return = train_equity[-1] / train_equity[0] - 1
        train_returns.append(train_return)

        test_bt = run_backtest(test_df, cloned_model, features)
        fold_results.append(test_bt)

        test_equity = np.array(test_bt['equity_curve'])
        test_return = test_equity[-1] / test_equity[0] - 1
        test_returns.append(test_return)

    train_returns = np.array(train_returns)
    test_returns = np.array(test_returns)
    wfe_list = test_returns / train_returns
    wfe = np.mean(wfe_list)

    return fold_results, wfe

# === 17. ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ =====
def monte_carlo_simulation(equity_curve, n_simulations=1000):
    final_returns = []
    max_drawdowns = []

    for _ in range(n_simulations):
        shuffled_returns = np.random.permutation(np.diff(equity_curve) / equity_curve[:-1])
        sim_curve = [1.0]
        for r in shuffled_returns:
            sim_curve.append(sim_curve[-1] * (1 + r))

        sim_curve = np.array(sim_curve)
        final_return = sim_curve[-1] - 1
        drawdown = sim_curve / np.maximum.accumulate(sim_curve) - 1
        max_dd = drawdown.min()

        final_returns.append(final_return)
        max_drawdowns.append(max_dd)

    return np.array(final_returns), np.array(max_drawdowns)

# === 18. ã‚¢ã‚¦ãƒˆã‚ªãƒ–ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚¹ãƒˆ =====
def run_oos_test(df, model_class, features, train_ratio=0.8):
    n_train = int(len(df) * train_ratio)
    train_df = df.iloc[:n_train]
    oos_df = df.iloc[n_train:]

    model = model_class()
    model.fit(train_df[features], train_df['long_target'])

    oos_bt = run_backtest(oos_df, model, features)

    return oos_bt

# === 19. ãƒ­ãƒã‚¹ãƒˆæœ€é©åŒ–ã‚¹ã‚³ã‚¢è¨ˆç®— =====
def calculate_robust_score(fold_results):
    sharpe_ratios = [r['sharpe_ratio'] for r in fold_results]
    mean_sr = np.mean(sharpe_ratios)
    std_sr = np.std(sharpe_ratios)
    robust_score = mean_sr - std_sr
    return robust_score

# === 17. ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœã®ãƒ—ãƒ­ãƒƒãƒˆ ===def plot_capital_curve(backtest_results):
def plot_capital_curve(backtest_results):
    plt.figure(figsize=(10, 5))
    plt.plot(backtest_results['equity_curve'], label='Equity Curve')
    plt.title('è³‡ç”£æ¨ç§»')
    plt.xlabel('æœŸé–“')
    plt.ylabel('è³‡ç”£')
    plt.legend()
    plt.grid()
    plt.show()

# === 18. ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœã®è¡¨ç¤º ===
def display_backtest_results(results):
    print("\nãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœ:")
    print(f"{'é …ç›®':<15} {'å€¤':>10}")
    print(f"{'-'*25}")
    for key, value in results.items():
        if isinstance(value, list):  # å€¤ãŒãƒªã‚¹ãƒˆã®å ´åˆ
            print(f"{key:<15} {len(value):>10} (ãƒªã‚¹ãƒˆã®é•·ã•)")
        elif isinstance(value, (int, float)):  # å€¤ãŒæ•°å€¤ã®å ´åˆ
            print(f"{key:<15} {value:>10.2f}")
        else:  # ãã®ä»–ã®å‹ã®å ´åˆ
            print(f"{key:<15} {str(value):>10}")

# === 19. ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœã®ä¿å­˜ ===
def save_backtest_results(backtest_results, filename="backtest_results.csv"):
    # equity_curve ã®é•·ã•ã«åˆã‚ã›ã¦ä»–ã®åˆ—ã‚’åŸ‹ã‚ã‚‹
    equity_curve_length = len(backtest_results["equity_curve"])
    equity_df = pd.DataFrame({
        "equity_curve": backtest_results["equity_curve"],
        "total_pnl": [backtest_results["total_pnl"]] * equity_curve_length,
        "win_rate": [backtest_results["win_rate"]] * equity_curve_length,
        "sharpe_ratio": [backtest_results["sharpe_ratio"]] * equity_curve_length,
        "max_drawdown": [backtest_results["max_drawdown"]] * equity_curve_length
    })
    equity_df.to_csv(filename, index=False)
    print(f"ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœã‚’ {filename} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

# === 20. ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰åˆ†æçµæœã®ä¿å­˜ ===
def save_walk_forward_results(walk_forward_results, filename="walk_forward_results.csv"):
    # å„æœŸé–“ã®çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›
    results_list = []
    for i, result in enumerate(walk_forward_results):
        results_list.append({
            "fold": i + 1,
            "total_pnl": result["total_pnl"],
            "win_rate": result["win_rate"],
            "sharpe_ratio": result["sharpe_ratio"],
            "max_drawdown": result["max_drawdown"]
        })
    results_df = pd.DataFrame(results_list)
    results_df.to_csv(filename, index=False)
    print(f"ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰åˆ†æçµæœã‚’ {filename} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

# Optuna ã®ãƒ­ã‚¬ãƒ¼ã‚’å–å¾—ã—ã¦ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’è¨­å®š
optuna_logger = optuna.logging.get_logger("optuna")
optuna_logger.setLevel(logging.WARNING)  # INFO ãƒ­ã‚°ã‚’éè¡¨ç¤ºã«ã™ã‚‹

# === 21. ãƒ¡ã‚¤ãƒ³é–¢æ•° ===
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import TimeSeriesSplit
from sklearn.base import clone
from tqdm import tqdm
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import StackingClassifier, VotingClassifier
from sklearn.neural_network import MLPClassifier

# å¿…è¦ãªè‡ªä½œé–¢æ•°ï¼ˆget_data, basic_preprocessing, calc_features, etc.ï¼‰ã¯åˆ¥é€”importã•ã‚Œã¦ã„ã‚‹å‰æã§ã™

def main():
    start_time = time.time()

    steps = [
        "ãƒ‡ãƒ¼ã‚¿å–å¾—",
        "å‰å‡¦ç†ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼‰",
        "ç‰¹å¾´é‡ç”Ÿæˆ",
        "ç›¸é–¢ä¿‚æ•°ã«ã‚ˆã‚‹ç‰¹å¾´é‡å‰Šé™¤",
        "ç´¯ç©ãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®—",
        "ç‰¹å¾´é‡é¸æŠ",
        "ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–",
        "ãƒ¢ãƒ‡ãƒ«ä½œæˆ",
        "å„ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°",
        "ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä½œæˆ",
        "ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ",
        "ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœä¿å­˜",
        "ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰åˆ†æ",
        "ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰çµæœä¿å­˜",
        "ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³",
        "ãƒ­ãƒã‚¹ãƒˆæœ€é©åŒ–",
        "tæ¤œå®š",
        "på¹³å‡æ³•",
        "å¯è¦–åŒ–"
    ]

    with tqdm(total=len(steps), desc="é€²æ—çŠ¶æ³") as pbar:
        # ãƒ‡ãƒ¼ã‚¿å–å¾—
        df = get_data(TICKER, START_DATE, END_DATE, INTERVAL)
        if df is None or df.empty:
            print("ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            return
        pbar.update(1)

        # å‰å‡¦ç†
        df = basic_preprocessing(df)
        pbar.update(1)

        # ç‰¹å¾´é‡ç”Ÿæˆ
        df = calc_features(df)
        pbar.update(1)

        # ç›¸é–¢é™¤å»
        df = remove_highly_correlated_features(df, exclude_columns=['close', 'close_scaled'])
        pbar.update(1)

        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        FEATURES = [col for col in df.columns if col not in ['long_target', 'close', 'close_scaled']]
        scaler = MinMaxScaler()
        df[FEATURES] = scaler.fit_transform(df[FEATURES])
        pbar.update(1)

        # ç´¯ç©ãƒªã‚¿ãƒ¼ãƒ³è¨ˆç®—
        df = add_cumret(df)
        pbar.update(1)

        # ç‰¹å¾´é‡é¸æŠ
        top_features = select_top_features_with_lightgbm(df, target='long_target', top_n=10)
        FEATURES = top_features
        print(f"ğŸ” LightGBMé¸æŠœç‰¹å¾´é‡: {FEATURES}")
        pbar.update(1)

        # ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–
        best_params_dict = {model: optimize_hyperparameters(df, model) for model in SELECTED_MODELS}
        pbar.update(1)

        # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
        base_models = create_base_models(SELECTED_MODELS, best_params_dict)
        pbar.update(1)

        # å„ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
        for name, model in base_models:
            model.fit(df[FEATURES], df['long_target'])
        pbar.update(1)

        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä½œæˆ
        if ENSEMBLE_TYPE == 'stacking':
            ensemble_model = StackingClassifier(estimators=base_models, final_estimator=MLPClassifier(max_iter=1000))
        elif ENSEMBLE_TYPE == 'voting_hard':
            ensemble_model = VotingClassifier(estimators=base_models, voting='hard')
        elif ENSEMBLE_TYPE == 'voting_soft':
            ensemble_model = VotingClassifier(estimators=base_models, voting='soft')
        else:
            raise ValueError(f"Unsupported ensemble type: {ENSEMBLE_TYPE}")
        pbar.update(1)

        # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        ensemble_model.fit(df[FEATURES], df['long_target'])
        backtest_results = run_backtest(df, ensemble_model, FEATURES)
        pbar.update(1)

        # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœä¿å­˜
        display_backtest_results(backtest_results)
        save_backtest_results(backtest_results, filename="backtest_results.csv")
        pbar.update(1)

        # ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰åˆ†æ
        fold_results, wfe = run_walk_forward_backtest(df, ensemble_model, FEATURES, n_splits=5)
        print(f"ğŸ›« ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰åŠ¹ç‡ (WFE): {wfe:.4f}")
        pbar.update(1)

        # ã‚¦ã‚©ãƒ¼ã‚¯ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰çµæœä¿å­˜
        save_walk_forward_results(fold_results, filename="walk_forward_results.csv")
        pbar.update(1)

        # ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        mc_mean, mc_std = monte_carlo_simulation(df['cum_ret'], n_simulations=1000)
        print(f"ğŸ² ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­çµæœ: å¹³å‡æœ€çµ‚ãƒªã‚¿ãƒ¼ãƒ³={mc_mean[0]:.4f}, æ¨™æº–åå·®={mc_std[0]:.4f}")
        pbar.update(1)

        # ãƒ­ãƒã‚¹ãƒˆæœ€é©åŒ–
        robustness_score = calculate_robust_score(fold_results)
        print(f"ğŸ›¡ï¸ ãƒ­ãƒã‚¹ãƒˆãƒã‚¹ã‚¹ã‚³ã‚¢: {robustness_score:.4f}")
        pbar.update(1)

        # tæ¤œå®š
        t_stat, p_value = perform_t_test(df)
        print(f"ğŸ§ª tæ¤œå®šçµæœ: tå€¤={t_stat:.4f}, på€¤={p_value:.4f}")
        pbar.update(1)

        # på¹³å‡æ³•
        x = df['cum_ret'].diff(1).dropna()
        period = 14
        alpha = 0.03
        mean_p, significant, error_rate = p_mean_test(x, period=period, alpha=alpha)
        print(f"ğŸ“ˆ på¹³å‡æ³•çµæœ: å¹³å‡på€¤={mean_p:.4f}, æœ‰æ„ã‹ã©ã†ã‹={significant}, ã‚¨ãƒ©ãƒ¼ç‡={error_rate:.4e}")
        pbar.update(1)

        # å¯è¦–åŒ–
        plot_equity_curve(df)
        plot_feature_heatmap(df, FEATURES)
        pbar.update(1)

    end_time = time.time()
    print('========================================================')
    print(f"âœ… å®Œäº†ï¼å…¨ä½“ã®å®Ÿè¡Œæ™‚é–“: {end_time - start_time:.2f} ç§’")

if __name__ == "__main__":
    main()
